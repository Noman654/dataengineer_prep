{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noman654/dataengineer_prep/blob/main/Spark/syntax_practical/common_asked_syntax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_zK1bFyjZV0"
      },
      "source": [
        "## Lets Focus on what question and try to understand the syntax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Nwj0byFjjZV1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# setup the spark\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Data Abstractions Quick Reference\n",
        "\n",
        "1. RDD (Resilient Distributed Dataset)\n",
        "   - Low-level foundation with direct memory control\n",
        "   - Best for unstructured data and custom processing\n",
        "   - Available in all languages\n",
        "\n",
        "2. DataFrame\n",
        "   - High-level, optimized API for structured data\n",
        "   - SQL-like operations and optimizations\n",
        "   - Works across all Spark languages\n",
        "   - Recommended default choice\n",
        "\n",
        "3. Dataset\n",
        "   - Combines DataFrame optimization with RDD type safety\n",
        "   - Only available in Scala/Java\n",
        "   - Not available in Python/R\n",
        "\n",
        "Note: Use DataFrame as default choice unless you need:\n",
        "- Low-level control → Use RDD\n",
        "- Compile-time type safety (Scala/Java) → Use Dataset"
      ],
      "metadata": {
        "id": "PMz8XEFhoQeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now lets see how many ways to create Dataframe"
      ],
      "metadata": {
        "id": "DIcrq7D9oft4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. first one using tuple to make rdd and use rdd to make dataframe\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")])\n",
        "columns = [\"ID\", \"Name\"]\n",
        "\n",
        "# if we did'nt pass columns  they give you random name c0_1 like and schema is blank so bydefault  it take inferschema = True\n",
        "df = spark.createDataFrame(rdd, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoSSV6mylMkX",
        "outputId": "efbfedc0-9afd-4de1-f6d9-c9f9150e0f98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. another way by using only tuple\n",
        "\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "\n",
        "columns = [\"ID\", \"Name\"]\n",
        "df = spark.createDataFrame(rdd, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js1y6pkpo9Xn",
        "outputId": "377250bc-e631-41cf-8005-fb2b0783e32a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. way using dict in this way you dont have to pass columns\n",
        "\n",
        "data = [{\"id\": 1, \"name\": \"Alice\", \"marks\":[50,40,30,30,40]},\n",
        " {\"id\": 2, \"name\": \"Bob\" , \"marks\":[50,40,30,50,50,100,120]},\n",
        "  {\"id\": 3, \"name\": \"Charlie\", \"marks\":[50,40,30,20,20,10]}]\n",
        "\n",
        "# in this we are using a list of marks so it will make  a array for each rows by default with long type if we did'nt pass schema\n",
        "df = spark.createDataFrame(data)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1uwHYu5pW1p",
        "outputId": "e4ce833c-a61d-48d6-8524-4a10c484f332"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-------+\n",
            "| id|               marks|   name|\n",
            "+---+--------------------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|\n",
            "+---+--------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3DpkzyyJXp",
        "outputId": "693f1117-7c3b-4250-907f-3c2f926427ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- marks: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some important type you should focus\n",
        "1. **Array** *its like the list in python but in this list all element should be same type *\n",
        "2. **Struct**"
      ],
      "metadata": {
        "id": "eDKvTfyep0BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see one more example with schema\n",
        "schema = T.StructType(\n",
        "   [ T.StructField(\"id\",T.IntegerType()),\n",
        "    T.StructField(\"name\",T.StringType()),\n",
        "    T.StructField(\"marks\",T.ArrayType(T.IntegerType()))\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.printSchema() # if you see now element in array is long"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9v6TQIdskRr",
        "outputId": "d657b7ec-0b5c-4572-c62a-2ee5aa66724c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- marks: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of UDFs in Spark\n",
        "- Mostly ask question if you are writing some pyspark code on interview."
      ],
      "metadata": {
        "id": "nTqnrLOIzQKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Standard UDF\n",
        "- **Performance**: Slower due to Python-JVM serialization.\n",
        "- Note : by default return type is `String`"
      ],
      "metadata": {
        "id": "e3i_0wGh2Au-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def multiply(x):\n",
        "    return x * 2\n",
        "\n",
        "multiply_udf = udf(multiply, IntegerType())\n",
        "\n",
        "# another way you can use decorator\n",
        "# @udf(IntegerType())\n",
        "# def multiply_udf(x):\n",
        "#     return x * 2\n",
        "\n",
        "df.withColumn(\"new_col\", multiply_udf(df[\"id\"])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OejB9--6VaL",
        "outputId": "bdece720-fa35-4023-8302-676c415a7fb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-------+-------+\n",
            "| id|               marks|   name|new_col|\n",
            "+---+--------------------+-------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|      2|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|      4|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|      6|\n",
            "+---+--------------------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Pandas UDFs (Vectorized)\n",
        "----\n",
        "- **Performance**: Faster with Arrow optimizations.\n",
        "- Note : if custom logic is required, prefer Pandas UDFs ***over standard UDFs for better performance***\n",
        "-----\n",
        "**Cons:**\n",
        "Why Not Always Use Pandas UDFs?\n",
        "- Memory-intensive for large datasets.\n",
        "- Requires PyArrow and Pandas setup.\n",
        "- Not suitable for non-vectorized operations.\n"
      ],
      "metadata": {
        "id": "16a1TG7G7F6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def multiply(x):\n",
        "    return x * 2\n",
        "\n",
        "multiply_udf = pandas_udf(multiply, IntegerType())\n",
        "\n",
        "# another way you can use decorator\n",
        "# @pandas_udf(IntegerType())\n",
        "# def multiply_udf(x):\n",
        "#     return x * 2\n",
        "\n",
        "df.withColumn(\"new_col\", multiply_udf(df[\"id\"])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MupGID0Y6Vf9",
        "outputId": "2d909ff5-b79f-4c6e-e6af-70ab7f9f3678"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-------+-------+\n",
            "| id|               marks|   name|new_col|\n",
            "+---+--------------------+-------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|      2|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|      4|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|      6|\n",
            "+---+--------------------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 SQL-Based UDF\n",
        "----\n",
        "- **Definition**: Register UDFs for SQL queries.\n"
      ],
      "metadata": {
        "id": "BmBBDlyR7h47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")\n"
      ],
      "metadata": {
        "id": "f1KwkmrU6VkR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"my_udf\", lambda x: x + ' Gupta')\n",
        "spark.sql(\"SELECT *, my_udf(name) AS new_column FROM people\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jlc0fEo16tb",
        "outputId": "231a9573-8c2e-4716-bb11-fa0d49db895d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-------+-------------+\n",
            "| id|               marks|   name|   new_column|\n",
            "+---+--------------------+-------+-------------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|  Alice Gupta|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|    Bob Gupta|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|Charlie Gupta|\n",
            "+---+--------------------+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oC_9rrOS73_B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "EAl0J_TVpxeh",
        "outputId": "37d9d98e-c69e-4abd-d376-8ea3e6d0fada"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sqlContext' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4d22189764d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_udf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Gupta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT my_udf(column_name) AS new_column FROM people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "zzJq83WdjZV2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dn8NluYgjbU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}