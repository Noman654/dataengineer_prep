{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noman654/dataengineer_prep/blob/main/Spark/syntax_practical/common_asked_syntax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_zK1bFyjZV0"
      },
      "source": [
        "# Important Syntax and Function You Should Review Before a PySpark Interview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nwj0byFjjZV1",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# setup the spark\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMz8XEFhoQeV"
      },
      "source": [
        "### Spark Data Abstractions Quick Reference\n",
        "\n",
        "1. RDD (Resilient Distributed Dataset)\n",
        "   - Low-level foundation with direct memory control\n",
        "   - Best for unstructured data and custom processing\n",
        "   - Available in all languages\n",
        "\n",
        "2. DataFrame\n",
        "   - High-level, optimized API for structured data\n",
        "   - SQL-like operations and optimizations\n",
        "   - Works across all Spark languages\n",
        "   - Recommended default choice\n",
        "\n",
        "3. Dataset\n",
        "   - Combines DataFrame optimization with RDD type safety\n",
        "   - Only available in Scala/Java\n",
        "   - Not available in Python/R\n",
        "\n",
        "Note: Use DataFrame as default choice unless you need:\n",
        "- Low-level control → Use RDD\n",
        "- Compile-time type safety (Scala/Java) → Use Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIcrq7D9oft4"
      },
      "source": [
        "### Now lets see how many ways to create Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoSSV6mylMkX",
        "outputId": "efbfedc0-9afd-4de1-f6d9-c9f9150e0f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. first one using tuple to make rdd and use rdd to make dataframe\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")])\n",
        "columns = [\"ID\", \"Name\"]\n",
        "\n",
        "# if we did'nt pass columns  they give you random name c0_1 like and schema is blank so bydefault  it take inferschema = True\n",
        "df = spark.createDataFrame(rdd, columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js1y6pkpo9Xn",
        "outputId": "377250bc-e631-41cf-8005-fb2b0783e32a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. another way by using only tuple\n",
        "\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "\n",
        "columns = [\"ID\", \"Name\"]\n",
        "df = spark.createDataFrame(rdd, columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1uwHYu5pW1p",
        "outputId": "e4ce833c-a61d-48d6-8524-4a10c484f332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+-------+\n",
            "| id|               marks|   name|\n",
            "+---+--------------------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|\n",
            "+---+--------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. way using dict in this way you dont have to pass columns\n",
        "\n",
        "data = [{\"id\": 1, \"name\": \"Alice\", \"marks\":[50,40,30,30,40]},\n",
        " {\"id\": 2, \"name\": \"Bob\" , \"marks\":[50,40,30,50,50,100,120]},\n",
        "  {\"id\": 3, \"name\": \"Charlie\", \"marks\":[50,40,30,20,20,10]}]\n",
        "\n",
        "# in this we are using a list of marks so it will make  a array for each rows by default with long type if we did'nt pass schema\n",
        "df = spark.createDataFrame(data)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3DpkzyyJXp",
        "outputId": "693f1117-7c3b-4250-907f-3c2f926427ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- marks: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDKvTfyep0BW"
      },
      "source": [
        "### Some important type you should focus\n",
        "1. **Array** its like the list in python but in this list all element should be same type\n",
        "2. **Struct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9v6TQIdskRr",
        "outputId": "d657b7ec-0b5c-4572-c62a-2ee5aa66724c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- marks: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# lets see one more example with schema\n",
        "schema = T.StructType(\n",
        "   [ T.StructField(\"id\",T.IntegerType()),\n",
        "    T.StructField(\"name\",T.StringType()),\n",
        "    T.StructField(\"marks\",T.ArrayType(T.IntegerType()))\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.printSchema() # if you see now element in array is long"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTqnrLOIzQKV"
      },
      "source": [
        "# Types of UDFs in Spark\n",
        "- Mostly ask question if you are writing some pyspark code on interview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3i_0wGh2Au-"
      },
      "source": [
        "#### 1.1 Standard UDF\n",
        "- **Performance**: Slower due to Python-JVM serialization.\n",
        "- Note : by default return type is `String`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OejB9--6VaL",
        "outputId": "bdece720-fa35-4023-8302-676c415a7fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+-------+-------+\n",
            "| id|               marks|   name|new_col|\n",
            "+---+--------------------+-------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|      2|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|      4|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|      6|\n",
            "+---+--------------------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def multiply(x):\n",
        "    return x * 2\n",
        "\n",
        "multiply_udf = udf(multiply, IntegerType())\n",
        "\n",
        "# another way you can use decorator\n",
        "# @udf(IntegerType())\n",
        "# def multiply_udf(x):\n",
        "#     return x * 2\n",
        "\n",
        "df.withColumn(\"new_col\", multiply_udf(df[\"id\"])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a1TG7G7F6u"
      },
      "source": [
        "#### 1.2 Pandas UDFs (Vectorized)\n",
        "----\n",
        "- **Performance**: Faster with Arrow optimizations.\n",
        "- Note : if custom logic is required, prefer Pandas UDFs ***over standard UDFs for better performance***\n",
        "-----\n",
        "**Cons:**\n",
        "Why Not Always Use Pandas UDFs?\n",
        "- Memory-intensive for large datasets.\n",
        "- Requires PyArrow and Pandas setup.\n",
        "- Not suitable for non-vectorized operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MupGID0Y6Vf9",
        "outputId": "2d909ff5-b79f-4c6e-e6af-70ab7f9f3678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+-------+-------+\n",
            "| id|               marks|   name|new_col|\n",
            "+---+--------------------+-------+-------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|      2|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|      4|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|      6|\n",
            "+---+--------------------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def multiply(x):\n",
        "    return x * 2\n",
        "\n",
        "multiply_udf = pandas_udf(multiply, IntegerType())\n",
        "\n",
        "# another way you can use decorator\n",
        "# @pandas_udf(IntegerType())\n",
        "# def multiply_udf(x):\n",
        "#     return x * 2\n",
        "\n",
        "df.withColumn(\"new_col\", multiply_udf(df[\"id\"])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmBBDlyR7h47"
      },
      "source": [
        "#### 1.3 SQL-Based UDF\n",
        "----\n",
        "- **Definition**: Register UDFs for SQL queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "f1KwkmrU6VkR"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"people\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jlc0fEo16tb",
        "outputId": "231a9573-8c2e-4716-bb11-fa0d49db895d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+-------+-------------+\n",
            "| id|               marks|   name|   new_column|\n",
            "+---+--------------------+-------+-------------+\n",
            "|  1|[50, 40, 30, 30, 40]|  Alice|  Alice Gupta|\n",
            "|  2|[50, 40, 30, 50, ...|    Bob|    Bob Gupta|\n",
            "|  3|[50, 40, 30, 20, ...|Charlie|Charlie Gupta|\n",
            "+---+--------------------+-------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.udf.register(\"my_udf\", lambda x: x + ' Gupta')\n",
        "spark.sql(\"SELECT *, my_udf(name) AS new_column FROM people\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC_9rrOS73_B"
      },
      "source": [
        "## Cache Vs Persist\n",
        "#### **Why Are Cache and Persist Needed?** \n",
        "- **Recomputations Are Expensive:** In Spark, transformations like map() and filter() are lazily evaluated. Without caching or persisting, *these transformations are recomputed every time an action (e.g., count, collect) is called*.\n",
        "- **Performance Optimization:** Storing intermediate results in memory or disk reduces computation time, especially for iterative tasks like machine learning and graph processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAl0J_TVpxeh"
      },
      "source": [
        "### **1. Cache**\n",
        "The cache() method stores intermediate results in memory (**default: MEMORY_AND_DISK**) for reuse across multiple actions.\n",
        "\n",
        "#### **Key Points:**\n",
        "- **Default Storage Level:** Data is stored in memory first; if memory is insufficient, it spills to disk.\n",
        "- **Lazy Evaluation:** Data is cached **only when an action is triggered** (e.g., count(), collect()).\n",
        "- **Simplicity:** cache() is equivalent to persist(StorageLevel.MEMORY_AND_DISK) and does not allow customization of the storage level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzJq83WdjZV2",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn8NluYgjbU-"
      },
      "source": [
        "### **2. Persist**\n",
        "The persist() method allows more flexibility by enabling custom storage levels for storing intermediate results.\n",
        "\n",
        "**Key Points:**\n",
        "- **Customizable Storage:** Supports multiple storage levels, such as:\n",
        "  - **MEMORY_ONLY**: Stores data only in memory. If memory is insufficient, recomputation occurs.\n",
        "  - **DISK_ONLY**: Stores data only on disk.\n",
        "  - **MEMORY_AND_DISK_SER**: Stores serialized data to save memory, spilling to disk if necessary.\n",
        "- **Lazy Evaluation:** Like cache(), it’s triggered **only during actions.**\n",
        "- **Advanced Scenarios:** Use persist() when you need specific storage levels for resource optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "df.persist(StorageLevel.DISK_ONLY)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
